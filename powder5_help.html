<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Powder Pattern Decomposition - Help</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <script>
        MathJax = {
          tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']]
          },
          svg: {
            fontCache: 'global'
          }
        };
    </script>
    <script type="text/javascript" id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
    </script>

    <style>
        :root {
            --system-blue: #007AFF;
            --system-gray-1: #8E8E93;
            --system-gray-2: #AEAEB2;
            --system-gray-3: #C7C7CC;
            --system-gray-4: #D1D1D6;
            --system-gray-5: #E5E5EA;
            --system-gray-6: #F2F2F7;
            --system-background: #FFFFFF;
            --system-grouped-background: #F2F2F7;
            --system-label: #000000;
            --system-secondary-label: rgba(60, 60, 67, 0.6);
            --system-separator: rgba(60, 60, 67, 0.29);
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Inter', sans-serif;
            margin: 0;
            background-color: var(--system-grouped-background);
            display: flex;
            flex-direction: column;
            height: 100vh;
            color: var(--system-label);
            line-height: 1.7;
            font-size: 16px;
        }

        #app-container {
            display: flex;
            width: 100%;
            height: 100%;
            overflow: hidden;
        }

        #nav-panel {
            width: 280px;
            min-width: 240px;
            flex-shrink: 0;
            padding: 24px;
            background-color: var(--system-background);
            border-right: 1px solid var(--system-separator);
            overflow-y: auto;
        }

        #nav-panel h2 {
            font-size: 18px;
            margin-top: 0;
            margin-bottom: 24px;
            color: var(--system-label);
        }

        #nav-panel ul {
            list-style: none;
            padding: 0;
            margin: 0;
        }

        #nav-panel li a {
            display: block;
            padding: 8px 12px;
            text-decoration: none;
            color: var(--system-secondary-label);
            border-radius: 8px;
            font-weight: 500;
            transition: all 0.2s ease;
            font-size: 15px;
        }

        #nav-panel li a.sub-link {
            padding-left: 28px;
            font-size: 14px;
            font-weight: 400;
        }

        #nav-panel li a:hover {
            background-color: var(--system-gray-6);
            color: var(--system-label);
        }

        #nav-panel li a.active {
            background-color: var(--system-blue);
            color: white;
            font-weight: 600;
        }


        #content-area {
            flex-grow: 1;
            padding: 32px 48px;
            overflow-y: auto;
            background-color: var(--system-background);
        }

        #content-area h2 {
            font-size: 28px;
            font-weight: 700;
            margin-top: 0;
            padding-bottom: 16px;
            border-bottom: 1px solid var(--system-separator);
            margin-bottom: 24px;
        }

        #content-area h3 {
            font-size: 22px;
            font-weight: 600;
            margin-top: 40px;
            margin-bottom: 16px;
            padding-bottom: 8px;
            border-bottom: 1px solid var(--system-gray-5);
        }

        #content-area h4 {
            font-size: 18px;
            font-weight: 600;
            margin-top: 24px;
            margin-bottom: 12px;
            color: var(--system-secondary-label);
        }

        code {
            background-color: var(--system-gray-6);
            padding: 2px 6px;
            border-radius: 4px;
            font-family: 'SF Mono', 'Menlo', 'monospace';
            font-size: 0.9em;
            border: 1px solid var(--system-gray-4);
        }

        .reference-item {
            margin-bottom: 1em;
        }

        .note {
            background-color: #eef6ff; /* Light blue background */
            border-left: 4px solid var(--system-blue);
            padding: 16px;
            margin: 24px 0;
            border-radius: 0 8px 8px 0;
        }
        /* --- Added Warning Note Style --- */
        .warning-note {
            background-color: #fffbeb; /* Light yellow background */
            border-left: 4px solid #f59e0b; /* Amber/Yellow border */
            padding: 16px;
            margin: 24px 0;
            border-radius: 0 8px 8px 0;
        }
        .warning-note strong {
             color: #b45309; /* Darker amber text */
        }
        /* Added style for links in About section */
        #about a {
            color: var(--system-blue);
            text-decoration: none;
        }
        #about a:hover {
            text-decoration: underline;
        }
    </style>
</head>
<body>

<div id="app-container">
    <nav id="nav-panel">
        <h2>Help Topics</h2>
        <ul id="nav-links">
            <li><a href="#introduction">Introduction</a></li>
            <li><a href="#getting-started">Getting Started</a></li>
            <li><a href="#chart-interactions">Chart Interactions</a></li>
            <li><a href="#decomposition-methods">Decomposition Methods</a></li>
            <li><a href="#le-bail-method" class="sub-link">Le Bail Method</a></li>
            <li><a href="#pawley-method" class="sub-link">Pawley Method</a></li>
            <li><a href="#algorithms">Refinement Algorithms</a></li>
            <li><a href="#algorithms-lm" class="sub-link">Levenberg-Marquardt</a></li>
            <li><a href="#algorithms-sa" class="sub-link">Simulated Annealing</a></li>
            <li><a href="#algorithms-pt" class="sub-link">Parallel Tempering</a></li>
            <li><a href="#parameters">Parameter Guide</a></li>
            <li><a href="#parameters-crystal" class="sub-link">Crystal & Space Group</a></li>
            <li><a href="#parameters-instrumental" class="sub-link">Instrumental</a></li>
            <li><a href="#parameters-background" class="sub-link">Background (Spline)</a></li> <li><a href="#parameters-profile4" class="sub-link">Simple pVoigt</a></li>
            <li><a href="#parameters-profile3" class="sub-link">TCH (Anisotropic)</a></li>
            <li><a href="#parameters-profile-split" class="sub-link">Split pVoigt (Asymmetric)</a></li>
            <li><a href="#strategy">Refinement Strategy</a></li>
            <li><a href="#results">Interpreting Results</a></li>
            <blockquote>
              <li><a href="#results-wh" class="sub-link">Williamson-Hall Analysis</a></li>
            </blockquote>
            <li><a href="#references">References</a></li>
            <li><a href="#about">About</a></li>
        </ul>
    </nav>

    <main id="content-area">
        <section id="introduction">
            <h2>Introduction</h2>
            <p>
                This document serves as a scientific and technical guide for the <strong>powder5</strong> web application, a tool for the analysis of powder X-ray diffraction (PXRD) data via whole-pattern fitting. This technique, also known as pattern decomposition, is a crucial method in materials science and crystallography for refining structural and microstructural parameters when a complete structural model is either unknown or unnecessary.
            </p>
            <p>
                The application facilitates the extraction of precise lattice parameters, peak profile information, and integrated intensities of Bragg reflections. It implements two principal decomposition algorithms: the iterative <strong>Le Bail method</strong> for rapid and stable convergence, and the simultaneous <strong>Pawley method</strong> for rigorous, unbiased intensity extraction. Peak profiles are modeled using phenomenological functions, including a versatile <strong>Simple pseudo-Voigt</strong>, an <strong>Asymmetric Split pseudo-Voigt</strong>, and a physically rigorous <strong>Anisotropic model (TCH)</strong> based on the Thompson-Cox-Hastings formulation with a Stephens model for anisotropic line broadening. The background is modeled using linear interpolation between user-defined points.
            </p>
        </section>

        <hr>

        <section id="getting-started">
            <h2>Getting Started: Data Input</h2>
            <p>
                Analysis commences with the loading of a powder diffraction data file. The application is designed to automatically parse numerous common ASCII-based file formats from major instrument manufacturers and standard crystallographic software.
            </p>
            <ul>
                <li><b>Supported Formats:</b> Built-in parsers are included for Bruker (<code>.brml</code>, <code>.uxd</code>), PANalytical (<code>.xrdml</code>), Rigaku (<code>.ras</code>, <code>.rasx</code>), Philips (<code>.udf</code>, <code>.rd</code>, <code>.sd</code>), GSAS (<code>.esd</code>, <code>.gsa</code>, <code>.xra</code>), and Jade (<code>.mdi</code>).</li>
                <li><b>Generic Data:</b> Standard two-column ASCII files (<code>.xy</code>, <code>.csv</code>, <code>.txt</code>, <code>.dat</code>, <code>.asc</code> etc.) containing $2\theta$ and intensity values are also supported. The parser accommodates space, comma, or semicolon delimiters. Comment lines prefixed with <code>#</code>, <code>!</code>, <code>;</code> or <code>//</code> are ignored, as are non-numeric header lines.</li>
                <li><b>Metadata Parsing:</b> For many instrument-specific formats, instrument parameters such as the X-ray wavelength for Kα1 are read from the file's metadata and used to populate the relevant fields in the user interface. It is incumbent upon the user to verify the correctness of these automatically populated values.</li>
            </ul>
        </section>

        <hr>

        <section id="chart-interactions">
            <h2>Interactive Data Visualization</h2>
            <p>The diffraction pattern is rendered in an interactive plot to facilitate detailed inspection of the experimental data and the quality of the model fit. You can hide any of the plots by clicking on its legend at the top of the chart.</p>
            <ul>
                <li><b>Pan:</b> Click and drag on the chart to translate the view along the $2\theta$ and intensity axes.</li>
                <li><b>Zoom:</b> Use the mouse wheel to adjust the zoom level. The zoom behavior is context-dependent:
                    <ul>
                        <li><b>Over the plot area:</b> Zooms both axes isotropically.</li>
                        <li><b>Over the Intensity (Y) axis:</b> Zooms the vertical axis exclusively.</li>
                        <li><b>Over the $2\theta$ (X) axis:</b> Zooms the horizontal axis exclusively.</li>
                    </ul>
                </li>
                <li><b>Reset View:</b> A <strong>right-click</strong> on the chart resets the zoom and pan to the range currently defined by the <strong>2θ Min/Max sliders</strong>.</li>
                <li><b>Reflection Data:</b> Hovering the cursor near a Bragg reflection marker (tick mark) displays a tooltip containing the corresponding Miller indices ($hkl$).</li>
                <li><b>Add Background Spline Point:</b> To add a point defining the background shape, hold down the <strong>Ctrl</strong> key and <strong>click</strong> on a point in the pattern. This adds the nearest experimental data point to the background spline list. Points cannot be added outside the current <strong>2θ Min/Max slider</strong> range, nor can they be added exactly at the edge point indices.</li> </ul>
        </section>

        <hr>

        <section id="decomposition-methods">
            <h2>Pattern Decomposition Methodologies</h2>
            <p>
               Pattern decomposition enables the fitting of a powder diffraction pattern based on a unit cell and space group, without requiring a full structural model (atomic coordinates). This is essential for the precise determination of lattice parameters and the extraction of integrated intensities, which are requisite for ab initio structure determination.
            </p>
            <div class="note">
                <strong>A Note on Intensity:</strong> In pattern decomposition, the "intensity" parameter ($I_{hkl}$) of a Bragg reflection refers to its total <strong>integrated area</strong>, not its maximum peak height. The program correctly normalizes the calculated peak shape function by its mathematical area, ensuring that the refined $I_{hkl}$ parameter is a physically meaningful quantity representing the total scattering power of a reflection.
            </div>
            <h3 id="le-bail-method">The Le Bail Method</h3>
            <p>
                The Le Bail method is an iterative, sequential algorithm known for its computational efficiency and robust convergence. The process is as follows:
            </p>
            <ol>
                <li><b>Initialization:</b> A theoretical pattern is calculated from the user-supplied lattice, profile, and background parameters (defined by the spline points). The integrated intensities of all reflections ($I_{hkl}$) are initially assumed to be equal.</li> <li><b>Intensity Extraction:</b> The observed intensity at each data point ($y_{i,obs}$) is partitioned among the calculated Bragg peaks that contribute to that point. The contribution of each peak is proportional to its profile function value at that point. Summing these partitioned intensities for each reflection yields a new set of "observed" integrated intensities.</li>
                <li><b>Parameter Refinement:</b> These newly extracted intensities are held constant, and a non-linear least-squares refinement is performed on the lattice and profile parameters (the background is fixed by the spline points) to minimize the difference between the calculated and observed patterns.</li> <li><b>Iteration:</b> The refined parameters from Step 3 are used to restart the process from Step 1. This cycle repeats until the parameters and R-factors converge.</li>
            </ol>

            <h3 id="pawley-method">The Pawley Method</h3>
            <p>
                The Pawley method employs a simultaneous, non-iterative approach. It treats the integrated intensity ($I_{hkl}$) of each Bragg reflection as an independent, refinable variable within a single, large-scale least-squares minimization.
            </p>
            <p>
                This means that all parameters—lattice, profile, and all individual integrated intensities—are refined concurrently. The background shape is fixed by the user-defined spline points and is not refined. </p>
            <ul>
                <li><b>Advantages:</b> The Pawley method is considered more rigorous as it avoids the iterative bias of the Le Bail method, particularly in cases of severe peak overlap. It can yield more accurate and statistically sound integrated intensities and parameter uncertainties.</li>
                <li><b>Disadvantages:</b> The inclusion of hundreds or thousands of intensity variables significantly increases the computational complexity and memory requirements. The refinement is also more susceptible to instability and parameter correlation, especially if the initial model is poor or if stochastic optimization algorithms are used.</li>
            </ul>
             <div class="warning-note">
                <strong>Algorithm Recommendation for Pawley Method:</strong><br>
                Due to the high dimensionality and potential for parameter correlation when refining individual intensities, the Pawley method is significantly more stable when used with the <strong>Levenberg-Marquardt (LM)</strong> algorithm. The stochastic algorithms (Simulated Annealing and Parallel Tempering) can struggle to converge reliably in Pawley mode. <strong>It is strongly recommended to use only the LM algorithm for Pawley refinements, preferably after obtaining a good initial model using the Le Bail method.</strong> The user interface enforces this by disabling the "Run Pawley" button when SA or PT is selected.
            </div>
             </section>

        <hr>

        <section id="algorithms">
            <h2>Minimization Algorithms</h2>
            <p>The goal of refinement is to minimize the sum-of-squares objective function, $\chi^2 = \sum w_i (y_{i,obs} - y_{i,calc})^2$, where $w_i$ is the statistical weight of each data point (typically $w_i = 1/y_{i,obs}$). This application provides several algorithms to navigate the complex parameter space and find the minimum of this function.</p>

            <h3 id="algorithms-lm">Levenberg-Marquardt (LM)</h3>
            <p>
                The LM algorithm is a standard gradient-based method for non-linear least-squares problems. It effectively interpolates between the Gauss-Newton algorithm and the method of gradient descent. By calculating the Jacobian matrix (the matrix of first partial derivatives of the calculated pattern with respect to the parameters), it determines the most efficient path toward the nearest local minimum.
            </p>
            <ul>
                <li><b>Characteristics:</b> LM is a local minimizer, exhibiting rapid quadratic convergence when the initial parameters are close to the true minimum. It is the preferred method for final, high-precision refinement and is the only algorithm here that can calculate valid estimated standard deviations (ESDs) for the refined parameters from the covariance matrix.</li>
                <li><b>Limitations:</b> It is susceptible to converging to a local minimum if the starting model is far from the global solution.</li>
                <li><b>Pawley Mode: Recommended algorithm for Pawley refinements due to stability.</b></li>
            </ul>

            <h3 id="algorithms-sa">Simulated Annealing (SA)</h3>
            <p>
                Simulated Annealing is a stochastic, global optimization metaheuristic inspired by the annealing process in metallurgy. The algorithm introduces a "temperature" parameter, $T$, which controls the probability of accepting a parameter step that increases the objective function.
            </p>
            <p>
                 At high temperatures, the algorithm explores the parameter space widely, frequently accepting "uphill" moves that allow it to escape local minima. As the refinement proceeds, $T$ is gradually reduced according to a cooling schedule, reducing the probability of accepting non-improving solutions. The system eventually "freezes" into a low-energy state, which is expected to be the global minimum.
            </p>
             <ul>
                <li><b>Use Case:</b> Ideal for exploring complex solution spaces or when the initial parameters are poorly known, particularly in Le Bail mode. It can effectively escape local minima where gradient-based methods like LM would become trapped.</li>
                <li><b>Pawley Mode: Not recommended due to potential instability.</b></li>
            </ul>

            <h3 id="algorithms-pt">Parallel Tempering (Replica Exchange)</h3>
            <p>
                Parallel Tempering, also known as Replica Exchange MCMC, is an advanced stochastic algorithm designed to overcome the slow convergence of traditional Simulated Annealing. Instead of a single system with a decreasing temperature, Parallel Tempering simulates multiple copies (or "replicas") of the system simultaneously, each at a different, fixed temperature in a predefined ladder ($T_1 < T_2 < ... < T_N$).
            </p>
            <ul>
                <li><b>Mechanism:</b> Each replica evolves independently according to a standard Monte Carlo or SA algorithm at its respective temperature. The high-temperature replicas explore the parameter space broadly (high mobility), while the low-temperature replicas perform a fine-grained search of local minima (high precision).</li>
                <li><b>The Swap Move:</b> Periodically, the algorithm attempts to swap the entire set of parameters between adjacent replicas (e.g., between replica $i$ at temperature $T_i$ and replica $i+1$ at $T_{i+1}$). The swap is accepted with a Metropolis-like probability that depends on the energies and temperatures of the two replicas. This crucial step allows a good solution discovered by a high-temperature replica in a distant valley to "percolate down" to the low-temperature replicas, dramatically improving the efficiency of finding the global minimum.</li>
                 <li><b>Advantages:</b> Significantly more efficient at global exploration than standard Simulated Annealing, making it the most robust choice for complex problems or when the starting model is highly uncertain (primarily in Le Bail mode).</li>
                 <li><b>Pawley Mode: Not recommended due to potential instability.</b></li>
            </ul>
        </section>

        <hr>

        <section id="parameters">
            <h2>Guide to Refinable Parameters</h2>
            <p>This section provides a detailed breakdown of the parameters you can control and refine.</p>

            <div class="note">
                <h4>A Note on Parameter Scaling & GSAS Comparison</h4>
                <p>
                    Following a long-standing convention in crystallographic software like GSAS, some refinable parameters in this program are internally scaled. This is done for user convenience, allowing you to work with manageable numbers (e.g., 1.0) instead of very small decimals (e.g., 1.0e-4). The documentation below provides the exact formulas used, allowing for direct comparison with physical models and values from other software.
                </p>
            </div>

            <h3 id="parameters-crystal">Crystal System & Space Group</h3>
            <p>
                These parameters define the crystallographic symmetry of the material.
            </p>
            <ul>
                <li>The <strong>System</strong> selection imposes metrical constraints on the lattice parameters (e.g., for Cubic, $a=b=c$, $\alpha=\beta=\gamma=90^\circ$).</li>
                <li>The <strong>Space Group</strong> selection determines the systematic reflection conditions ($hkl$ absences) used to generate the list of Bragg peaks. The underlying logic for these conditions is consistent with established crystallographic libraries and was taken from Computational Crystallography Toolbox (cctbx).</li>
            </ul>

            <h3 id="parameters-instrumental">Instrumental Parameters</h3>
            <p>
                Found under the "Sample" tab, these parameters model the diffractometer configuration.
            </p>
            <ul>
                <li><code>Radiation 1/2 (Å) & Ratio</code>: Defines the X-ray source. For divergent-beam laboratory instruments, a Kα1/Kα2 doublet is typically used. For synchrotron radiation, the Ratio is set to 0.</li>
                <li><code>Zero</code>: A refinable parameter that corrects for instrumental zero-point error in the $2\theta$ axis. It is highly correlated with lattice parameters and must be refined with caution.</li>
                <li><code>2θ Min / Max</code>: These sliders define the refinement range. It is standard practice to exclude regions of low signal-to-noise or known artifacts from the calculation.</li>
            </ul>

            <h3 id="parameters-background">Background Modeling (Spline Interpolation)</h3>
            <p>
                The background contribution is modeled using linear interpolation between a series of user-defined points (spline points or knots). This approach provides flexibility without introducing refinable background parameters into the least-squares minimization. The background shape is therefore considered <strong>fixed</strong> during the refinement process.
            </p>
            <p>
                Control of the background spline is located under the "Background" tab:
            </p>
             <ul>
                <li><b>Auto-generation:</b> Use the <strong>Auto-points slider</strong> (10-40 points) and the <strong>Generate Spline Points</strong> button to automatically populate the list. The algorithm selects points corresponding to local intensity minima within intervals distributed across the current <strong>2θ Min/Max slider</strong> range. The points at the exact 2θ Min and Max slider positions are always included and fixed to these $2\theta$ values.</li>
                <li><b>Manual Addition:</b> Add individual points by holding <strong>Ctrl</strong> and clicking on the chart. The closest experimental point will be added to the list, provided it's within the current slider range and not an edge point.</li>
                <li><b>Editable List:</b> The generated and manually added points appear in a list below the controls.
                    <ul>
                        <li>You can directly edit the <strong>$2\theta$</strong> and <strong>Intensity</strong> values for any point, except for the $2\theta$ values of the first (Min) and last (Max) points, which are fixed by the sliders.</li>
                        <li>Points can be deleted using the <strong>&times;</strong> button, except for the first and last points.</li>
                    </ul>
                </li>
                 <li><b>Chart Display:</b> The spline points can be toggled on/off on the chart using the "Show Points on Chart" checkbox. The calculated spline curve is always shown.</li>
             </ul>
             <div class="note">
                 <strong>Important:</strong> Define the background points carefully <strong>before</strong> starting the Le Bail or Pawley refinement. Since these points are not refined parameters, their positions directly determine the background shape subtracted during the fit.
             </div>
            <h3 id="parameters-profile4">Simple pVoigt</h3>
            <p>
                This function models the peak shape as a linear combination of a Gaussian and a Lorentzian function: $pV(x) = \eta L(x) + (1-\eta)G(x)$. The angular dependence of the Full Width at Half Maximum (FWHM) for each component is modeled empirically.
                $$H_G^2 = GU \tan^2\theta + GV \tan\theta + GW + GP / \cos^2\theta$$
                $$H_L = LX / \cos\theta$$
            </p>
            <ul>
                <li><code>GU, GV, GW, GP</code>: Parameters describing the Gaussian FWHM ($H_G$). The terms are associated with strain ($GU$), instrumental factors ($GV, GW$), and particle size effects ($GP$). Note that $GU$, $GW$, and $GP$ should physically be non-negative.</li>
                <li><code>LX</code>: Describes the Lorentzian FWHM ($H_L$), primarily associated with crystallite size broadening ($LX > 0$).</li>
                <li><code>eta</code>: A simple linear mixing parameter ($0 \le \eta \le 1$; $\eta=0$ for pure Gaussian, $\eta=1$ for pure Lorentzian).</li>
                <li><code>shft & trns</code>: Corrects for peak position shifts due to sample displacement and transparency, respectively.
                    <div class="note" style="margin-top: 1em; margin-bottom: 0;">
                        <strong>Unit and Scaling for the <code>shft</code> parameter:</strong><br>
                        The refined <code>shft</code> parameter is a <strong>dimensionless, scaled coefficient</strong>, not a direct physical length. Its relationship to the physical specimen displacement ($s$) and the goniometer radius ($R$) is defined as follows:
                        <ul>
                            <li>The physical peak shift in radians is: $\Delta(2\theta)_{\text{rad}} = -2 \frac{s}{R} \cos(\theta)$</li>
                            <li>The program calculates this shift (in degrees) using the formula: $\Delta(2\theta)_{\text{deg}} = -(\text{shft} / 1000) \times \cos(\theta) \times (180 / \pi)$</li>
                            <li>Therefore, the relationship is: $\frac{\text{shft}}{1000} = \frac{2s}{R}$</li>
                            <li>To find the physical displacement $s$ from the refined parameter, use: $s = R \times (\text{shft} / 2000)$.<br>
                            <em>Example: For a typical instrument with $R=240$ mm, a refined <code>shft</code> value of 1.0 corresponds to a physical displacement $s$ of $240 \times (1/2000) = 0.12$ mm.</em></li>
                        </ul>
                    </div>
                </li>
            </ul>

            <h3 id="parameters-profile3">TCH (Size/Strain/Aniso)</h3>
            <p>
                This is a physically rigorous profile function adapted from GSAS, convoluting a <strong>Thompson-Cox-Hastings (TCH) pseudo-Voigt</strong> with a <strong>Stephens model</strong> for anisotropic strain broadening.
            </p>

            <h4>Isotropic Broadening (TCH Model)</h4>
            <p>The TCH formulation models the FWHM of the Gaussian ($H_G$) and Lorentzian ($H_L$) components based on physical contributions to line broadening:
                $$H_G^2 = U \tan^2\theta + V \tan\theta + W$$
                $$H_L = X \tan\theta + Y / \cos\theta$$
            </p>
            <p>The total FWHM ($H$) and pseudo-Voigt mixing parameter ($\eta$) are then derived from these components using polynomial approximations. The final shape is $pV(x) = \eta L(x, H) + (1-\eta)G(x, H)$, where both functions share the same convoluted FWHM.</p>
            <ul>
                <li><code>U, V, W</code>: Gaussian broadening parameters related to strain ($U, V$) and instrumental resolution ($W$). Physically, $U$ and $W$ should be non-negative.</li>
                <li><code>X, Y</code>: Lorentzian broadening parameters related to strain ($X$) and crystallite size ($Y$). Physically, $X$ and $Y$ should be non-negative.</li>
            </ul>

            <h4>Peak Asymmetry</h4>
             <p>The <code>S/L</code> and <code>H/L</code> parameters introduce an angle-dependent asymmetry, primarily correcting for axial divergence effects at low $2\theta$.</p>

            <h4>Anisotropic Broadening (Stephens Model)</h4>
            <p>
                Anisotropic microstrain, where broadening varies with crystallographic direction, is modeled by adding terms to the Lorentzian component ($H_L$) that are dependent on the Miller indices ($hkl$). The model is a fourth-order polynomial in the reciprocal lattice vectors.
            </p>
            <p>
                The refinable parameters (<code>S400</code>, <code>S040</code>, etc.) are the non-zero, symmetry-unique coefficients of this polynomial. The application automatically applies symmetry constraints based on the Laue class of the selected space group (e.g., for cubic, $S400=S040=S004$).
            </p>
            <div class="note">
                <strong>Unit and Scaling for Stephens <code>S_hkl</code> parameters:</strong><br>
                The user-inputted <code>S_hkl</code> parameters are scaled for convenience. The dimensionless anisotropic broadening term ($H_{aniso}$) is calculated from these parameters, and its contribution to the total Lorentzian width (in degrees $2\theta$) is scaled by a factor of 1000.
                $$H_L(\text{total}) = H_L(\text{isotropic}) + \frac{|H_{aniso}|}{1000}$$
                This scaling allows the user to refine values in a manageable range (e.g., -10 to +10) rather than requiring input of very small decimals (e.g., 1e-4), a convention common in other refinement software.
            </div>
        </section> <section id="parameters-profile-split"> <h3 id="parameters-profile-split">Split pVoigt (Asymmetric)</h3>
            <p>
                This profile function is a modification of the Simple pseudo-Voigt designed to model asymmetric peaks (e.g., from axial divergence or stacking faults). It achieves this by defining independent sets of profile width parameters for the left side (at $2\theta$ values less than the peak center) and the right side of the peak.
            </p>
            <p>
                The shape is still a linear combination $pV(x) = \eta L(x) + (1-\eta)G(x)$, but the $H_G$ and $H_L$ parameters used in the calculation depend on whether $x$ is to the left or right of the peak center.
            </p>

            <h4>Gaussian Broadening (Left & Right)</h4>
            <p>The Gaussian FWHM ($H_G$) for each side is modeled as:</p>
            <p>$$H_{G, \text{side}}^2 = GU_{\text{side}} \tan^2\theta + GV_{\text{side}} \tan\theta + GW_{\text{side}}$$</p>
            <p>(Note: This model does not use the $GP$ term used in the Simple pVoigt profile.)</p>
            <ul>
                <li><code>GU-L, GV-L, GW-L</code>: Parameters describing the Gaussian FWHM for the <strong>left side</strong> of the peak.</li>
                <li><code>GU-R, GV-R, GW-R</code>: Parameters describing the Gaussian FWHM for the <strong>right side</strong> of the peak.</li>
            </ul>

            <h4>Lorentzian Broadening (Left & Right)</h4>
            <p>The Lorentzian FWHM ($H_L$) for each side is modeled as:</p>
            <p>$$H_{L, \text{side}} = LX_{\text{side}} / \cos\theta$$</p>
            <ul>
                <li><code>LX-L</code>: Describes the Lorentzian FWHM for the <strong>left side</strong>, primarily associated with size broadening.</li>
                <li><code>LX-R</code>: Describes the Lorentzian FWHM for the <strong>right side</strong>, primarily associated with size broadening.</li>
            </ul>

            <h4>Peak Shape & Position</h4>
            <ul>
                <li><code>eta (Mixing)</code> (param: <code>eta_split</code>): A simple linear mixing parameter ($0 \le \eta \le 1$; $\eta=0$ for pure Gaussian, $\eta=1$ for pure Lorentzian). This single value is used for both sides of the peak.</li>
                <li><code>shft (Displ.)</code> (param: <code>shft_split</code>): Corrects for peak position shifts due to sample displacement.</li>
                <li><code>trns (Transp.)</code> (param: <code>trns_split</code>): Corrects for peak position shifts due to transparency.
                    <div class="note" style="margin-top: 1em; margin-bottom: 0;">
                        <strong>Unit and Scaling for the <code>shft_split</code> parameter:</strong><br>
                        This parameter is a <strong>dimensionless, scaled coefficient</strong>, identical in function to the <code>shft</code> parameter in the Simple pVoigt profile.
                        <ul>
                            <li>The physical peak shift in radians is: $\Delta(2\theta)_{\text{rad}} = -2 \frac{s}{R} \cos(\theta)$</li>
                            <li>The program calculates this shift (in degrees) using the formula: $\Delta(2\theta)_{\text{deg}} = -(\text{shft\_split} / 1000) \times \cos(\theta) \times (180 / \pi)$</li>
                            <li>Therefore, the relationship is: $\frac{\text{shft\_split}}{1000} = \frac{2s}{R}$</li>
                            <li>To find the physical displacement $s$ from the refined parameter, use: $s = R \times (\text{shft\_split} / 2000)$.</li>
                        </ul>
                    </div>
                </li>
            </ul>
        </section>
        <hr>

        <section id="strategy">
            <h2>Recommended Refinement Strategy</h2>
            <p>
                A sequential and hierarchical refinement strategy is crucial for achieving a stable and physically meaningful solution. Attempting to refine all parameters simultaneously from a poor starting model will likely lead to divergence or convergence to a false minimum.
            </p>

            <h3>Phase 1: Initial Model Setup</h3>
            <ol>
                <li><b>Define the Model:</b> Load data, select the crystal system and space group, and define the refinement range using the <strong>$2\theta$ sliders</strong>.</li>
                <li><b>Set Background Points:</b> Use the controls under the "Background" tab (<strong>Generate Spline Points</strong>, <strong>Ctrl+Click</strong> on chart, or manual editing of the list) to define a set of points that reasonably follow the experimental background. This background shape is fixed during refinement.</li>
                <li><b>Peak Position Refinement:</b> Using the <strong>Le Bail</strong> and <strong>LM</strong> algorithms, refine only the <strong>Lattice Parameter(s)</strong> and, if necessary, the instrumental <strong>Zero Shift</strong>. The goal is to align the calculated Bragg positions with the observed peak maxima.</li>
            </ol>

            <h3>Phase 2: Peak Profile Refinement</h3>
            <ol start="4">
                <li><b>Isotropic Broadening:</b> Once positions are correct, refine the primary isotropic peak shape parameters (e.g., <strong>W, Y, U, X</strong> in TCH, or <strong>GW, LX</strong> in Simple/Split pVoigt). This will account for the dominant size and strain contributions.</li>
                <li><b>Asymmetry and Shape:</b> Introduce asymmetry parameters (<strong>S/L</strong> in TCH, <strong>shft/trns</strong> in Simple/Split pVoigt) if there is a clear misfit, particularly at low angles. Refine the mixing parameter (<strong>eta</strong>) if needed.</li>
                <li><b>Anisotropic Broadening (TCH only):</b> If systematic misfits remain (e.g., some peaks are consistently broader than the model), introduce the anisotropic Stephens parameters (e.g., <strong>S400</strong>). Refine only the symmetry-independent terms.</li>
            </ol>

            <h3>Phase 3: Finalization and Intensity Extraction</h3>
            <ol start="7">
                 <li><b>Global Optimization (Optional):</b> If the LM algorithm converges to a poor solution during the Le Bail steps, switch to <strong>Parallel Tempering</strong> or <strong>Simulated Annealing</strong> for one or more Le Bail cycles to perform a global search. Afterwards, switch back to LM for a final, precise local minimization.</li>
                 <li><b>Pawley Refinement (LM Only):</b> With a stable and well-refined model from the Le Bail method, perform a final refinement using the <strong>Pawley method</strong> with the <strong>Levenberg-Marquardt (LM)</strong> algorithm. This will provide the most statistically robust set of integrated intensities, suitable for subsequent structure solution. <strong>Using SA or PT for Pawley is not recommended due to potential instability.</strong></li>
                 </ol>

                        <div class="note">
                <h4>Technical Note on Calculation Parameters</h4>
                <p>
                    The theoretical pattern is calculated using a hybrid approach for performance and accuracy. A search window (defined by <code>CALCULATION_WINDOW_MULTIPLIER</code>, typically 6-8 times the peak FWHM) is used to find all potential contributions to a given data point. However, only points where the calculated peak height is greater than a defined threshold (<code>PEAK_HEIGHT_CUTOFF</code>, currently 0.2%) relative to its maximum are included in the final sum.
                </p>
            </div>

        </section>

        <hr>

        <section id="results">
            <h2>Interpretation of Results</h2>
            <p>
                Assessing the quality of a refinement requires both statistical analysis and critical visual inspection of the fit.
            </p>

            <h3>Figures of Merit</h3>
            <p>Standard crystallographic R-factors are provided to quantify the quality of the fit.</p>
            <ul>
                <li>
                    <b>R-pattern ($R_p$):</b> The unweighted residual error based on net intensities, sensitive primarily to the fit of high-intensity reflections.
                    $$R_p = \frac{\sum |(y_{i,obs} - y_{i,bkg}) - (y_{i,calc} - y_{i,bkg})|}{\sum |y_{i,obs} - y_{i,bkg}|} \times 100\%$$
                </li>
                <li>
                    <b>Weighted R-pattern ($R_{wp}$):</b> The primary figure of merit, weighted by the inverse of the observed gross intensities ($w_i = 1/y_{i,obs}$), which properly accounts for the counting statistics across the entire pattern.
                    $$R_{wp} = \left[ \frac{\sum w_i (y_{i,obs} - y_{i,calc})^2}{\sum w_i y_{i,obs}^2} \right]^{1/2} \times 100\%$$
                </li>
                <li>
                    <b>Reduced Chi-squared ($\chi^2$, Goodness of Fit):</b> The most statistically rigorous indicator. For a statistically perfect fit where the model correctly describes the data and the weights are accurate, $\chi^2$ should approach 1.0.
                     $$\chi^2 = \frac{1}{N - P} \sum w_i (y_{i,obs} - y_{i,calc})^2 = \left(\frac{R_{wp}}{R_{exp}}\right)^2$$
                     where $N$ is the number of data points, $P$ is the number of refined parameters, and $R_{exp}$ is the statistically expected minimum $R_{wp}$.
                </li>
            </ul>

            <h4>Calculating Observed Intensities ($I_{obs}$) for Overlapping Peaks</h4>
            <p>
                A simple numerical integration over a fixed angular range is insufficient for accurately determining the observed integrated intensity ($I_{obs}$) of overlapping peaks. This tool employs a more robust <strong>intensity partitioning</strong> method.
            </p>
            <p>
                At each point in the diffraction pattern, the net observed intensity ($y_{obs} - y_{bkg}$) is distributed among all contributing Bragg reflections. This distribution is proportional to the value of each peak's calculated profile function at that specific point. By integrating these partitioned "slices" of intensity for each reflection across the entire pattern (using the trapezoidal rule), the method yields a reliable $I_{obs}$ value that correctly deconvolutes contributions from neighboring peaks.
            </p>

            <h3>Visual Inspection</h3>
            <p>
                Numerical indicators can be misleading. Visual inspection of the difference plot (observed minus calculated) is the most critical step in evaluating the fit.
            </p>
            <ul>
                <li>
                    <b>The Difference Plot:</b> A successful refinement should yield a difference plot that consists of random, uncorrelated noise centered on zero. The plot is scaled relative to the main pattern for visibility.
                </li>
                <li>
                    <b>Systematic Residuals:</b> The presence of structured, non-random features in the difference plot (e.g., "M-shaped" residuals around peaks, broad humps where the spline is inadequate, or un-indexed peaks) is a clear indication of systematic errors in the model. These may arise from an incorrect peak shape, unmodeled anisotropy or asymmetry, an inadequate background model (requiring adjustment of spline points), or the presence of an unaccounted-for impurity phase.</li> </ul>

            <blockquote>
            <h3 id="results-wh">Williamson-Hall Size-Strain Analysis</h3>
            <p>
                For refinements utilizing <strong>Profile Function #3 (Anisotropic TCH)</strong> and the <strong>Pawley method</strong>, the application can automatically perform a Williamson-Hall analysis to extract microstructural information. This method separates the contributions of crystallite size and microstrain to the total peak broadening by analyzing their different dependencies on the diffraction angle, $\theta$.
            </p>
            <p>
                The analysis is based on the linear Williamson-Hall equation, where $\beta$ is the total physical peak breadth (FWHM) in radians derived from the refined sample-only broadening parameters (<code>U</code>, <code>X</code>, <code>Y</code>):
                $$\beta \cos(\theta) = \frac{K\lambda}{L} + 4\epsilon \sin(\theta)$$
            </p>
            <p>
                This equation describes a straight line when plotting $\beta \cos(\theta)$ vs. $4\sin(\theta)$. The software performs a linear least-squares fit on the relevant data points to determine the y-intercept (related to crystallite size, $L$) and the slope (related to microstrain, $\epsilon$).
            </p>

            <h4>Reported Values</h4>
            <ul>
                <li><b>Apparent Crystallite Size (nm):</b> An estimate of the average size of the coherently scattering domains, calculated from the y-intercept of the Williamson-Hall plot (using $K=0.9$).</li>
                <li><b>Apparent Microstrain (%):</b> An estimate of the root-mean-square strain within the crystallites, calculated from the slope of the plot.</li>
                <li><b>Linear Fit R²:</b> The coefficient of determination for the linear regression. A value close to 1.0 indicates that the isotropic size/strain model is a good fit for the observed peak broadening. Values significantly less than 1.0 may suggest that broadening is anisotropic or that the model is otherwise inadequate.</li>
            </ul>
            </blockquote>

            <h3>Data Export</h3>
            <ul>
                <li><b>Save Report:</b> Generates a comprehensive ASCII text file containing all statistical indicators, refined parameter values with their ESDs (for LM refinements), the Williamson-Hall analysis results (if applicable), the list of background spline points used, a list of reflections with calculated and observed intensities, and (optionally) a point-by-point list of observed, calculated, background, and difference intensities.</li> <li><b>Generate PDF:</b> Creates a summary PDF document containing a high-resolution plot and tables of final parameters and statistics, suitable for archival or reporting.</li>
            </ul>
        </section>

        <hr>

        <section id="references">
            <h2>References & Further Reading</h2>
             <div class="reference-item">
                <strong>Pawley Method:</strong><br>
                Pawley, G. S. (1981). "Unit-cell refinement from powder diffraction scans". <em>Journal of Applied Crystallography</em>, 14(6), 357-361.
            </div>
            <div class="reference-item">
                <strong>Le Bail Method:</strong><br>
                Le Bail, A., Duroy, H. & Fourquet, J.L. (1988). "Ab-initio structure determination of LiSbWO6 by X-ray powder diffraction". <em>Materials Research Bulletin</em>, 23(3), 447-452.
            </div>
            <div class="reference-item">
                <strong>GSAS Profile Functions:</strong><br>
                Larson, A. C. & Von Dreele, R. B. (2004). "General Structure Analysis System (GSAS)". <em>Los Alamos National Laboratory Report LAUR 86-748</em>.
            </div>
             <div class="reference-item">
                <strong>TCH Profile Function:</strong><br>
                Thompson, P., Cox, D. E. & Hastings, J. B. (1987). "Rietveld refinement of Debye-Scherrer synchrotron X-ray data from Al2O3". <em>Journal of Applied Crystallography</em>, 20(2), 79-83.
            </div>
            <div class="reference-item">
                <strong>Stephens Anisotropy Model:</strong><br>
                Stephens, P. W. (1999). "Phenomenological model of anisotropic peak broadening in powder diffraction". <em>Journal of Applied Crystallography</em>, 32(2), 281-289.
            </div>
            <div class="reference-item">
                <strong>Parallel Tempering:</strong><br>
                Swendsen, R. H., & Wang, J. S. (1986). "Replica Monte Carlo simulation of spin-glasses". <em>Physical Review Letters</em>, 57(21), 2607.
            </div>
            <div class="reference-item">
                <strong>cctbx - Computational Crystallography Toolbox:</strong><br>
                Grosse-Kunstleve, R. W., Sauter, N. K., Moriarty, N. W., & Adams, P. D. (2002). "The Computational Crystallography Toolbox: crystallographic algorithms in a reusable software framework". <em>Journal of Applied Crystallography</em>, 35(1), 126-136. (Used for space group systematic absence rules).
            </div>
        </section>

        <hr>

        <section id="about">
            <h2>About This Tool</h2>
            <p>
                The <strong>powder5</strong> toolkit was developed by Nita Dragoe from Université Paris-Saclay as a simple browser-based implementation of powder pattern decomposition methods. It is a long-time successor of PowderV2 (Dragoe, N. (2001). <em>J. Appl. Cryst.</em>, 34, 535) and has been updated to include the Pawley method and modern global optimization algorithms.
            </p>

            <p>
                Numerical calculations, including matrix operations and optimization routines, are performed using the <a href="https://mathjs.org/" target="_blank">math.js</a> library, which is distributed under the Apache 2.0 license.
            </p>

            <p>
                Version: 1.15 | Last Updated: 27 October 2025.<br> This document was updated with the assistance of an AI.
            </p>
            <div class="note">
                <strong>Disclaimer:</strong> This application is provided for educational and research purposes. While it implements standard and robust algorithms, it is not a substitute for fully validated, peer-reviewed software packages (e.g., GSAS-II, FullProf, TOPAS) for analyses intended for publication.
            </div>
        </section>
    </main>
</div>

<script>
    // Simple script for active nav link highlighting on scroll
    document.addEventListener('DOMContentLoaded', () => {
        const contentArea = document.getElementById('content-area');
        const navLinks = document.querySelectorAll('#nav-links a');
        const sections = document.querySelectorAll('#content-area section');

        const activateLink = (id) => {
            navLinks.forEach(link => {
                link.classList.remove('active');
                if (link.getAttribute('href') === `#${id}`) {
                    link.classList.add('active');
                    // Optional: Scroll link into view if needed
                    // link.scrollIntoView({ behavior: 'smooth', block: 'nearest' });
                }
            });
        };

        // Debounce scroll listener for performance
        let scrollTimeout;
        contentArea.addEventListener('scroll', () => {
             clearTimeout(scrollTimeout);
             scrollTimeout = setTimeout(() => {
                let current = '';
                let minTop = Infinity;

                sections.forEach(section => {
                    const sectionTop = section.getBoundingClientRect().top; // Relative to viewport

                    // Find the section closest to the top of the viewport
                    if (sectionTop >= -100 && sectionTop < minTop) { // Allow slightly above viewport
                        minTop = sectionTop;
                        current = section.getAttribute('id');
                    }
                });

                // Fallback if no section is near the top (e.g., scrolled to bottom)
                if (!current && contentArea.scrollHeight - contentArea.scrollTop - contentArea.clientHeight < 100) {
                     current = sections[sections.length - 1].getAttribute('id');
                }

                if (current) {
                    activateLink(current);
                }
            }, 100); // Adjust debounce delay as needed
        });

         // Initial check on load
         if (sections.length > 0) {
              activateLink(sections[0].getAttribute('id'));
         }
    });
</script>

</body>
</html>